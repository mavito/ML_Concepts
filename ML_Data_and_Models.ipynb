{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM03zH/Gm//khxX8G7fL8X9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavito/ML_Concepts/blob/main/ML_Data_and_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data and Models in Machine Learning**\n",
        "\n"
      ],
      "metadata": {
        "id": "obDZgy9ku4hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Representing Objects as Vectors**\n",
        "\n",
        "Machine learning models cannot \"see\" objects; they can only process numerical attributes associated with those objects. Feature extraction is the process of selecting which attributes to measure. Once selected, these attributes form a \"Feature Vector.\" A collection of these vectors forms the Design Matrix.\n",
        "\n",
        "Imagine you are a chef categorizing different soups. You cannot feed the soup itself into a computer. Instead, you taste it and record specific numbers:\n",
        "\n",
        "- Saltiness (grams per liter)\n",
        "\n",
        "- Viscosity (thickness, 1-10 scale)\n",
        "\n",
        "- Temperature (Celsius)\n",
        "\n",
        "If you have a Tomato Soup with values [2.5, 4, 65] and a Gazpacho with values [3.0, 6, 4], you have converted the culinary reality of soup into vectors. If you write these on a chalkboard, one below the other, you have created a Design Matrix. The computer now understands \"soup\" as a coordinate in 3D space."
      ],
      "metadata": {
        "id": "af-au5oIYMR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Scenario: We are modeling \"Smartphones\".\n",
        "# We choose 3 features:\n",
        "# 1. Screen Size (inches)\n",
        "# 2. Battery Life (hours)\n",
        "# 3. Price (hundreds of dollars)\n",
        "\n",
        "# Phone 1: 6.1 inch, 18 hours, $800 (8.0)\n",
        "phone_1 = np.array([6.1, 18.0, 8.0])\n",
        "\n",
        "# Phone 2: 5.4 inch, 15 hours, $700 (7.0)\n",
        "phone_2 = np.array([5.4, 15.0, 7.0])\n",
        "\n",
        "# Phone 3: 6.7 inch, 22 hours, $1100 (11.0)\n",
        "phone_3 = np.array([6.7, 22.0, 11.0])\n",
        "\n",
        "# Creating the Design Matrix X\n",
        "# We stack the vectors vertically.\n",
        "# Shape will be (N=3 samples, D=3 features)\n",
        "X_phones = np.vstack([phone_1, phone_2, phone_3])\n",
        "\n",
        "print(\"Feature Vector for Phone 1:\", phone_1)\n",
        "print(\"\\nDesign Matrix X:\\n\", X_phones)\n",
        "print(\"\\nShape of X:\", X_phones.shape)\n",
        "# Output should indicate (3, 3)"
      ],
      "metadata": {
        "id": "ioDDlc_Xzrg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.1: Real Estate Properties\n",
        "\n",
        "1. Create a dataset representing \"Real Estate Properties.\"\n",
        "2. Define 4 properties.\n",
        "3. Select 3 features: ``.\n",
        "4. Create NumPy vectors for each property with realistic values.\n",
        "5. Stack them into a design matrix variable named X_real_estate.\n",
        "6. Print the matrix and its dimensions using .shape."
      ],
      "metadata": {
        "id": "6GSVaqVp0PWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "73VedPQskVWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Linear Function, Linear Model and Implementation**\n",
        "\n",
        "\n",
        "A linear model computes a weighted sum of inputs plus a bias (intercept). In vector notation, this is the dot product of the parameter vector $\\boldsymbol{\\theta}$ and the input vector $\\boldsymbol{x}$. The bias is often handled by appending a \"1\" to the input vector, allowing the entire operation to be a single dot product.\n",
        "\n",
        "Consider a simple model for taxi fare.\n",
        "- The Bias ($\\theta_0$): This is the base fare. It costs \\$3.00 just to step into the cab. Even if distance is 0, the cost is 3.00.\n",
        "- The Weight ($\\theta_1$): This is the rate per mile. Say, \\$2.00 per mile.\n",
        "- The Input ($x$): The distance traveled.\n",
        "\n",
        "If you travel 5 miles:$$\\text{Cost} = \\text{Base} + (\\text{Rate} \\times \\text{Distance})$$$$\\text{Cost} = 3.00 + (2.00 \\times 5) = 13.00$$Mathematically, we treat the input as x (the 1 represents the base condition) and the parameters as [3.00, 2.00]. The calculation is a dot product."
      ],
      "metadata": {
        "id": "sN_8X1Sj0Qe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_predict(x, theta):\n",
        "    \"\"\"\n",
        "    Computes y = theta. x\n",
        "    x: Input vector (D dimensions)\n",
        "    theta: Parameter vector (D dimensions)\n",
        "    \"\"\"\n",
        "    # The dot product sums the products of corresponding elements\n",
        "    return np.dot(theta, x)\n",
        "\n",
        "# Example: Taxi Ride\n",
        "# Parameters theta:\n",
        "theta_taxi = np.array([3.0, 2.0, 0.5])\n",
        "\n",
        "# Input x:\n",
        "# Note: We always add '1.0' as the first element to multiply with the Base Fare.\n",
        "ride_A = np.array([1.0, 10.0, 25.0]) # 10 miles, 25 minutes\n",
        "\n",
        "cost = linear_predict(ride_A, theta_taxi)\n",
        "\n",
        "print(f\"Parameters: {theta_taxi}\")\n",
        "print(f\"Input Features: {ride_A}\")\n",
        "print(f\"Predicted Cost: ${cost:.2f}\")\n",
        "# Calculation check: 3*1 + 2*10 + 0.5*25 = 3 + 20 + 12.5 = 35.5"
      ],
      "metadata": {
        "id": "i9BYj66PNMNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 Student Grades\n",
        "\n",
        "Implement a linear model to predict the grade of a student.\n",
        "\n",
        "1. Define an input vector x_student for a student who studied 8 hours and slept 6 hours. Remember to prepend a 1.0 for the bias.\n",
        "\n",
        "2. Define a parameter vector theta_grade.\n",
        "    - Base Score (Bias): 40 points.\n",
        "    - Points per Study Hour: 5 points.\n",
        "    - Points per Sleep Hour: 2 points.\n",
        "\n",
        "3. Use np.dot to calculate the predicted grade.\n",
        "\n",
        "4. Print the inputs, parameters, and final prediction."
      ],
      "metadata": {
        "id": "Q3ZFEP79NPqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Empirical Risk Minimization**\n",
        "\n",
        "How do we define a \"good\" model?\n",
        "\n",
        "In machine learning, we quantify \"badness\" using a Loss Function, denoted as $\\ell(y_n, \\hat{y}_n)$.\n",
        "\n",
        "This function measures the penalty for predicting $\\hat{y}_n$ when the true label is $y_n$.\n",
        "- Squared Loss (Regression): $\\ell(y, \\hat{y}) = (y - \\hat{y})^2$. Small errors are tolerated, but large errors are penalized quadratically.\n",
        "- 0-1 Loss (Classification): Returns 0 if correct, 1 if incorrect.  "
      ],
      "metadata": {
        "id": "YMqmHJpQ69zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ultimate goal is to minimize the True Risk ($R_{true}$), which is the expected loss over all possible data points the universe could ever generate.$$R_{true}(\\boldsymbol{\\theta}) = \\mathbb{E}_{x,y} [\\ell(y, f(\\boldsymbol{x}; \\boldsymbol{\\theta}))]$$However, we cannot compute this expectation because we do not have access to the infinite universe of data. We only have our finite training set $\\mathcal{D}$. Therefore, we minimize the Empirical Risk ($R_{emp}$), which is the average loss on the training data:"
      ],
      "metadata": {
        "id": "hp6tdKMe8uVc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Overfitting and Regularization**\n",
        "\n",
        "A model can \"cheat\" by memorizing the training data. Imagine a student who memorizes the answer key to a practice exam without understanding the subject. They will achieve 0% error on the practice exam (Empirical Risk = 0) but will fail the real exam (True Risk = High). This is called Overfitting.\n",
        "\n",
        "To prevent this, we introduce Regularization. We add a penalty term to our objective function that discourages the model from becoming too complex or having extremely large parameter values.$$\\min_{\\boldsymbol{\\theta}} R_{emp}(\\boldsymbol{\\theta}) + \\lambda \\|\\boldsymbol{\\theta}\\|^2$$This formulation seeks a compromise: fit the data well, but keep the solution simple."
      ],
      "metadata": {
        "id": "dn1SfWG9gBYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Squared Error Loss\n",
        "\n",
        "In regression, the most common metric for performance is the Mean Squared Error (MSE). It represents the Empirical Risk when using the Squared Loss function. By squaring the error, we ensure that positive and negative errors don't cancel each other out, and we place a higher penalty on outliers.\n",
        "\n",
        "You are throwing darts at a board. We want to measure your accuracy.\n",
        "1. Throw 1: You miss by 2 cm.\n",
        "2. Throw 2: You miss by -2 cm (2 cm to the left).\n",
        "\n",
        "If we just averaged the errors, $(2 + (-2)) / 2 = 0$. This would imply you are a perfect player, which is false!\n",
        "\n",
        "Instead, we square the errors: $2^2 = 4$ and $(-2)^2 = 4$. The average is $4$. This \"4\" is your Risk. To improve, you must get closer to the center."
      ],
      "metadata": {
        "id": "8LQPfErygbXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def compute_empirical_risk_mse(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes Empirical Risk using Mean Squared Error (MSE)\n",
        "    y_true: Vector of actual labels\n",
        "    y_pred: Vector of model predictions\n",
        "    \"\"\"\n",
        "    # 1. Compute the difference (residuals)\n",
        "    residuals = y_true - y_pred\n",
        "\n",
        "    # 2. Square the residuals\n",
        "    squared_errors = residuals ** 2\n",
        "\n",
        "    # 3. Compute the average (Empirical Risk)\n",
        "    risk = np.mean(squared_errors)\n",
        "\n",
        "    return risk\n",
        "\n",
        "# Example Data\n",
        "# True house prices (in millions)\n",
        "y_actual = np.array([1.5, 2.0, 1.2, 3.5])\n",
        "\n",
        "# Prediction Model A (A mediocre model)\n",
        "y_model_a = np.array([1.4, 2.2, 1.0, 3.0])\n",
        "\n",
        "# Prediction Model B (A better model)\n",
        "y_model_b = np.array([1.5, 2.0, 1.2, 3.4])\n",
        "\n",
        "risk_a = compute_empirical_risk_mse(y_actual, y_model_a)\n",
        "risk_b = compute_empirical_risk_mse(y_actual, y_model_b)\n",
        "\n",
        "print(f\"Risk for Model A: {risk_a:.4f}\")\n",
        "print(f\"Risk for Model B: {risk_b:.4f}\")\n",
        "# Model B should have lower risk (closer to 0)."
      ],
      "metadata": {
        "id": "wdmEXJVbk4IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1 :\n",
        "\n",
        "Calculate the risk for a \"Temperature Predictor.\"\n",
        "\n",
        "1. Create y_true_temp = [...].\n",
        "\n",
        "2. Create y_pred_temp = [...].\n",
        "\n",
        "3. Compute the Mean Squared Error using the logic above (don't just use a library function; write the steps to square and average).\n",
        "\n",
        "4. Print the result."
      ],
      "metadata": {
        "id": "OYbMNCOv89YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "GK5_VhXx9vak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Grid Search for Parameter Estimation**\n",
        "\n",
        "Now that we can calculate risk, how do we find the parameter $\\theta$ that minimizes it? One simple (brute-force) method is Grid Search. We define a range of possible values for $\\theta$, calculate the risk for each, and pick the best one. This helps visual the \"Loss Landscape.\"\n",
        "\n",
        "You are trying to tune a radio to a specific station, but the dial is unmarked. You want to find the spot with the clearest sound (minimum static noise).\n",
        "- You turn the dial to 88.0... lots of static.\n",
        "- You turn to 88.5... less static.\n",
        "- You turn to 89.0... crystal clear.\n",
        "- You turn to 89.5... static again.\n",
        "\n",
        "You choose 89.0 because it minimized the \"noise.\" Grid search does exactly this with mathematical parameters."
      ],
      "metadata": {
        "id": "DP4qdPANiqul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Create a synthetic dataset\n",
        "# The \"Truth\": y = 3.5 * x\n",
        "x_data = np.array([1,2,3,4,5])\n",
        "y_data = np.array([3.5, 7.0, 10.5, 14.0, 17.5])\n",
        "\n",
        "# 2. Define the \"Grid\" of parameters to test\n",
        "# We suspect the slope is between 0 and 6.\n",
        "candidate_thetas = np.linspace(0, 6, 100) # Test 100 different values\n",
        "\n",
        "risks =[]\n",
        "\n",
        "# 3. Iterate through every candidate\n",
        "for theta in candidate_thetas:\n",
        "    # Make prediction using this candidate theta\n",
        "    # Model: y = theta * x (Simple linear model, no bias for simplicity)\n",
        "    y_pred = theta * x_data\n",
        "\n",
        "    # Calculate Risk\n",
        "    risk = compute_empirical_risk_mse(y_data, y_pred)\n",
        "    risks.append(risk)\n",
        "\n",
        "# 4. Find the minimum risk\n",
        "min_risk_index = np.argmin(risks)\n",
        "best_theta = candidate_thetas[min_risk_index]\n",
        "\n",
        "print(f\"Best Theta found: {best_theta:.2f}\")\n",
        "\n",
        "# Visualization of the \"Loss Landscape\"\n",
        "plt.plot(candidate_thetas, risks)\n",
        "plt.scatter(best_theta, risks[min_risk_index], color='red', label='Minimum')\n",
        "plt.xlabel(\"Parameter Value (Theta)\")\n",
        "plt.ylabel(\"Empirical Risk (MSE)\")\n",
        "plt.title(\"Loss Landscape: Finding the Valley\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RQpJ1Zr-jD5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2 : Perform a grid search to find the best Intercept (Bias) for a dataset.\n",
        "\n",
        "1. Dataset:\n",
        "    - x = ,\n",
        "    - y = . *(The slope appears to be 1, but the intercept is unknown).*\n",
        "\n",
        "2. Assume the model is y = 1.0 * x + bias.\n",
        "\n",
        "3. Create a grid of candidate biases from 0 to 20.\n",
        "\n",
        "4. Loop through them, calculating the MSE for each.\n",
        "\n",
        "5. Print the bias that results in the lowest error."
      ],
      "metadata": {
        "id": "pH3Z09uLjRox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Parameter Estimation (Maximum Likelihood Estimation (MLE) & Maximum A Posteriori (MAP))**\n",
        "\n",
        "**4.1 Simulating Likelihood (Coin Toss)**\n",
        "\n",
        "The simplest probabilistic model is the Bernoulli distribution (coin flip). It has one parameter $\\mu$ (probability of heads). We will generate data using a \"secret\" $\\mu$, and then try to recover that $\\mu$ using MLE.\n",
        "\n",
        "Imagine a \"loaded\" die. You suspect it rolls '6' more often than it should. You cannot see the weight inside (the parameter). You can only roll it 100 times (generate data) and count the 6s. If you get 60 sixes, your best guess (MLE) for the probability of rolling a 6 is 0.60."
      ],
      "metadata": {
        "id": "IDrGBHgD97rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. The Generative Process (The \"Secret\")\n",
        "true_mu = 0.75 # The coin is biased to land Heads 75% of the time.\n",
        "\n",
        "# 2. Generating Data (The Experiment)\n",
        "# We flip the coin 50 times.\n",
        "N_flips = 50\n",
        "# np.random.binomial(n=1, p, size) simulates Bernoulli trials\n",
        "data_flips = np.random.binomial(n=1, p=true_mu, size=N_flips)\n",
        "\n",
        "print(\"Observed Data (1=Heads, 0=Tails):\")\n",
        "print(data_flips)\n",
        "\n",
        "# 3. Maximum Likelihood Estimation\n",
        "# For Bernoulli, the MLE is simply the sample mean (percentage of heads).\n",
        "estimated_mu_mle = np.mean(data_flips)\n",
        "\n",
        "print(f\"\\nTrue Parameter: {true_mu}\")\n",
        "print(f\"MLE Estimate:   {estimated_mu_mle}\")\n",
        "# Note: The estimate will likely not be exactly 0.75 due to randomness."
      ],
      "metadata": {
        "id": "UPoLS7av90pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4.1\n",
        "\n",
        "1. Simulate a \"Website Conversion Rate\" estimator.\n",
        "\n",
        "2. Set a hidden parameter true_conversion_rate = 0.12 (12% of visitors buy).\n",
        "\n",
        "3. Simulate N_visitors = 500. (Use np.random.binomial).\n",
        "\n",
        "4. Calculate the MLE estimate of the conversion rate from the data.\n",
        "\n",
        "5. Print the difference between the true rate and the estimated rate."
      ],
      "metadata": {
        "id": "LyOYulzo-YT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "karK3Y73-gO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.2 Implementing a Prior (MAP)**\n",
        "\n",
        "We will use a Beta distribution as a prior for the coin flip parameter $\\mu$, but for simplicity in this code, we will implement it as \"virtual data.\" Adding a prior that expects a fair coin is mathematically equivalent to adding some \"virtual\" heads and tails to our dataset before calculating the mean.\n",
        "\n",
        "Imagine you are playing cards with a stranger.\n",
        "- Data: He wins the first 3 hands.\n",
        "- MLE: \"He wins 100% of the time. He is a god of cards.\"\n",
        "- Prior: \"Most people win about 50% of the time.\"\n",
        "- MAP: You combine the data (3 wins) with your prior experience (imagine you've seen him play 10 virtual hands that were fair). The estimate shifts from 100% down to something more reasonable, like 60%. You are \"regularizing\" your judgment."
      ],
      "metadata": {
        "id": "msv-497s-s5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scenario: Small data sample\n",
        "# We flip a coin 5 times and get 5 Heads.\n",
        "data_small = np.array([1,1,1,1,1])\n",
        "\n",
        "# MLE Estimate\n",
        "mu_mle = np.mean(data_small)\n",
        "print(f\"MLE (Data only): {mu_mle}\")\n",
        "# Result is 1.0. We incorrectly believe Tails is impossible.\n",
        "\n",
        "# MAP Estimate (Adding a Prior)\n",
        "# Our Prior: We believe the coin is likely fair.\n",
        "# We represent this by adding \"pseudo-counts\"\n",
        "# alpha = 2 (virtual heads), beta = 2 (virtual tails)\n",
        "alpha_prior = 2\n",
        "beta_prior = 2\n",
        "\n",
        "num_heads = np.sum(data_small)\n",
        "num_total = len(data_small)\n",
        "\n",
        "# The formula for MAP with Beta prior is (heads + alpha - 1) / (total + alpha + beta - 2)\n",
        "# A simplified view is just adding counts:\n",
        "mu_map = (num_heads + alpha_prior) / (num_total + alpha_prior + beta_prior)\n",
        "\n",
        "print(f\"MAP (Data + Prior): {mu_map:.2f}\")\n",
        "# Result is 7/9 = 0.77.\n",
        "# We still think the coin is biased, but we no longer think Tails is impossible."
      ],
      "metadata": {
        "id": "ni3SJUv9mRbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 4.2\n",
        "\n",
        "You run a factory.\n",
        "\n",
        "1. Data: You test 3 products, and 0 are defective. (data = [0,0,0]).\n",
        "\n",
        "2. MLE: Calculate the MLE probability of a defect. (It will be 0.0).\n",
        "\n",
        "3. MAP: You know from history that defects do happen. Add a prior equivalent to seeing 1 defective item and 9 non-defective items in the past.\n",
        "\n",
        "4. Calculate the MAP estimate. (It should be non-zero)."
      ],
      "metadata": {
        "id": "MKp24rjJmMtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "g0zmCyVJrR0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Probabilistic Modeling and Inference**\n",
        "\n",
        "**5.1 Generative Process**\n",
        "\n",
        "A probabilistic model is essentially a story about how data is created. We call this the Generative Process. To understand a dataset, we try to reverse-engineer this process.\n",
        "\n",
        "Example: Linear Regression as a Generative Process.\n",
        "1. Pick a random input $\\boldsymbol{x}$.\n",
        "2. Compute the \"true\" value $\\boldsymbol{\\theta}^\\top \\boldsymbol{x}$.\n",
        "3. Add random Gaussian noise $\\epsilon$.\n",
        "4. Output $y = \\boldsymbol{\\theta}^\\top \\boldsymbol{x} + \\epsilon$.\n",
        "\n",
        "By defining this story mathematically, we can write down the likelihood and solve for $\\boldsymbol{\\theta}$."
      ],
      "metadata": {
        "id": "wKKW0QLU_JMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.2 Latent Variables**\n",
        "\n",
        "Sometimes, the data generation involves steps we cannot see. These invisible steps are controlled by Latent Variables, often denoted as $\\boldsymbol{z}$.\n",
        "\n",
        "Example: Gaussian Mixture Models (Chapter 11).\n",
        "\n",
        "Imagine measuring the height of people. The distribution looks like two humps (bimodal).\n",
        "- Latent Variable $z$: The gender of the person (Hidden/Unrecorded).\n",
        "- Observed Variable $x$: The height.\n",
        "\n",
        "The generative story is:\n",
        "1. Flip a coin (weighted by gender ratio) to choose $z$ (Male/Female).\n",
        "2. Sample height $x$ from the specific Gaussian distribution corresponding to $z$.Inference involves figuring out $z$ (Gender) given only $x$ (Height). This requires marginalizing (summing) over the possibilities of $z$."
      ],
      "metadata": {
        "id": "vvrAqN_ZWvlH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.3 Bayesian Inference**\n",
        "\n",
        "In Section 4.2, we used MAP to find the single best parameter set. Bayesian Inference goes further. Instead of finding a single value for $\\boldsymbol{\\theta}$, we want to calculate the full Posterior Distribution $p(\\boldsymbol{\\theta} | X, Y)$.\n",
        "\n",
        "We don't want to say \"The parameter is 5.\"\n",
        "\n",
        "We want to say \"The parameter is likely around 5, with a variance of 0.5.\"\n",
        "\n",
        "This allows us to quantify Uncertainty. When we make a prediction for a new point $x_{new}$, we average the predictions of all possible parameters, weighted by their probability. This is robust but computationally expensive."
      ],
      "metadata": {
        "id": "-Gf7tbayq5sx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To understand generative models, we will build one. We will create a synthetic dataset where the data comes from two different \"clusters\" (Latent Variable).\n",
        "\n",
        "Imagine a factory with two machines.\n",
        "\n",
        "- Machine A produces items weighing 100g on average.\n",
        "\n",
        "- Machine B produces items weighing 150g on average.\n",
        "The \"Latent Variable\" is which machine made the item. The \"Observed Variable\" is the weight. We will simulate this factory."
      ],
      "metadata": {
        "id": "VB6tTgIHrVKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# 1. Setup Parameters\n",
        "# Machine A parameters\n",
        "mu_A = 100\n",
        "sigma_A = 5\n",
        "\n",
        "# Machine B parameters\n",
        "mu_B = 150\n",
        "sigma_B = 10\n",
        "\n",
        "# Mixing probability (Latent Variable distribution)\n",
        "# 60% of items come from Machine A\n",
        "prob_A = 0.6\n",
        "\n",
        "# 2. Generate Data\n",
        "N_samples = 1000\n",
        "weights = []\n",
        "machines = []# We keep track of this for ground truth, but usually it's hidden.\n",
        "\n",
        "for _ in range(N_samples):\n",
        "    # Step 1: Choose Latent Variable (Machine A or B)\n",
        "    # random() gives a float between 0.0 and 1.0\n",
        "    if np.random.random() < prob_A:\n",
        "        chosen_machine = 'A'\n",
        "        # Step 2: Sample Observation from P(x | z=A)\n",
        "        w = np.random.normal(mu_A, sigma_A)\n",
        "    else:\n",
        "        chosen_machine = 'B'\n",
        "        # Step 2: Sample Observation from P(x | z=B)\n",
        "        w = np.random.normal(mu_B, sigma_B)\n",
        "\n",
        "    weights.append(w)\n",
        "    machines.append(chosen_machine)\n",
        "\n",
        "# 3. Visualize\n",
        "# This plot shows the \"Observed\" distribution (Histogram)\n",
        "sns.histplot(weights, bins=30, kde=True)\n",
        "plt.title(\"Observed Distribution of Weights (Mixture)\")\n",
        "plt.xlabel(\"Weight (g)\")\n",
        "plt.show()\n",
        "# You should see two 'humps' in the data."
      ],
      "metadata": {
        "id": "7pAcTR7K_f-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5:\n",
        "\n",
        "Simulate a \"School Test\" generative model.\n",
        "\n",
        "1. Two groups of students: \"Prepared\" and \"Unprepared\".\n",
        "\n",
        "2. Prob(Prepared) = 0.4.\n",
        "\n",
        "3. Prepared students score Normal(mean=85, std=5).\n",
        "\n",
        "4. Unprepared students score Normal(mean=60, std=10).\n",
        "\n",
        "5. Generate 500 scores.\n",
        "\n",
        "6. Plot the histogram. Can you see the two groups?"
      ],
      "metadata": {
        "id": "InOZKkvO_qdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "f8LUCDMbAIg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Directed Graphical Models**\n",
        "\n",
        "**6.1 Nodes and Edges**\n",
        "\n",
        "A Graphical Model represents a joint probability distribution as a graph.\n",
        "- Nodes (Circles): Represent Random Variables.\n",
        "  - Shaded Node: Observed variable (Data we have).\n",
        "  - Unshaded Node: Latent variable or Parameter (Unknowns we want to find)\n",
        "- Edges (Arrows): Represent Conditional Dependencies. An arrow $A \\rightarrow B$ implies the factor $p(B | A)$ exists in the joint distribution."
      ],
      "metadata": {
        "id": "OlD6zRAsAJko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.2 Plate Notation**\n",
        "\n",
        "In machine learning, we usually apply the same model to $N$ data points. Drawing $N$ identical nodes is tedious. We use Plate Notation: we draw a box (plate) around the repeating variables and place an $N$ in the corner.\n",
        "- Example: A coin flip model.\n",
        "- $\\mu$ (coin bias) is outside the plate (one parameter for the whole dataset).- $x_n$ (outcome) is inside the plate (repeats $N$ times).\n",
        "- Arrow $\\mu \\rightarrow x_n$."
      ],
      "metadata": {
        "id": "30FOnCCGsSgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.3 Conditional Independence (d-separation)**\n",
        "The graph structure reveals Conditional Independence. If we know the state of variable $B$, does learning about $A$ tell us anything new about $C$?Structure: $A \\rightarrow B \\rightarrow C$ (Chain).\n",
        "- If $B$ is unknown, $A$ and $C$ are dependent. (Rain causes Wet Ground causes Slipping).\n",
        "- If $B$ is observed (We know the ground is wet), $A$ and $C$ become independent. Knowing it rained doesn't add new info about slipping if we already know the ground is wet.This concept, called d-separation, allows us to simplify complex probability computations by dropping irrelevant variables."
      ],
      "metadata": {
        "id": "lffoUBcpsU2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the Python library networkx to draw a simple graphical model. This helps us visualize the logic: \"What depends on what?\"\n",
        "\n",
        "Consider the \"Wet Grass\" problem.\n",
        "\n",
        "- Variables: Rain, Sprinkler, Wet Grass.\n",
        "\n",
        "- Logic: Rain causes Wet Grass. Sprinkler causes Wet Grass. Rain influences Sprinkler (you turn it off if it rains).\n",
        "Graph: Rain -> Grass, Sprinkler -> Grass, Rain -> Sprinkler."
      ],
      "metadata": {
        "id": "HlsakY_jtG7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# 1. Initialize Directed Graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# 2. Add Nodes\n",
        "G.add_node(\"Rain\")\n",
        "G.add_node(\"Sprinkler\")\n",
        "G.add_node(\"Wet Grass\")\n",
        "\n",
        "# 3. Add Edges (Causal links)\n",
        "G.add_edge(\"Rain\", \"Wet Grass\")\n",
        "G.add_edge(\"Sprinkler\", \"Wet Grass\")\n",
        "G.add_edge(\"Rain\", \"Sprinkler\")\n",
        "\n",
        "# 4. Draw\n",
        "pos = nx.circular_layout(G)\n",
        "plt.figure(figsize=(6, 6))\n",
        "nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=3000, arrowsize=20)\n",
        "plt.title(\"Graphical Model: Wet Grass\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rypxr_SXrWQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6:\n",
        "\n",
        "Draw a graph for a \"Health Study.\"\n",
        "\n",
        "- Nodes: Diet, Exercise, Heart Health.\n",
        "\n",
        "- Edges:\n",
        "\n",
        "  - Diet affects Heart Health.\n",
        "\n",
        "  - Exercise affects Heart Health.\n",
        "\n",
        "  - Diet affects Exercise (Maybe better diet gives you energy to run?).\n",
        "\n",
        "- Generate the plot."
      ],
      "metadata": {
        "id": "FlsKwzRIBC9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "EPwZ5vpYBFDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Model Selection**\n",
        "\n",
        "How do we choose the right one? Generalization and Validation\n",
        "\n",
        "**7.1 The Bias-Variance Tradeoff**\n",
        "\n",
        "**Underfitting (High Bias):** The model is too simple (e.g., fitting a straight line to a parabola). It cannot capture the pattern.\n",
        "\n",
        "**Overfitting (High Variance):** The model is too complex. It connects every single data point, capturing noise rather than signal.\n",
        "\n",
        "We seek the \"Goldilocks\" zone in the middle."
      ],
      "metadata": {
        "id": "tiS9y5bCtcjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.2 Cross-Validation**\n",
        "\n",
        "We cannot judge a model's performance on the training data (due to overfitting risk). We need fresh data. Since data is expensive, we simulate fresh data using Cross-Validation.\n",
        "1. Split data into $K$ folds.\n",
        "2. Train on $K-1$ folds.\n",
        "3. Test on the held-out fold.\n",
        "4. Rotate and repeat $K$ times.\n",
        "5. Average the results.\n",
        "\n",
        "This gives a robust estimate of generalization error."
      ],
      "metadata": {
        "id": "a2571p52uI60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**7.3 Bayes Factors and Occam's Razor**\n",
        "\n",
        "Bayesian Model Selection offers a mathematically elegant solution called Automatic Occam's Razor. When comparing models using the Bayes Factor (ratio of marginal likelihoods), complex models are automatically penalized.\n",
        "\n",
        "A complex model spreads its probability mass over a huge variety of datasets. A simple model concentrates its mass on specific datasets. If the simple model can explain the data, its probability density at that point will be higher than the spread-out density of the complex model."
      ],
      "metadata": {
        "id": "DUmc-tsBuLK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation : K-Fold Cross-Validation**\n",
        "\n",
        "We will implement a simple validation loop to choose between a Linear Model (Degree 1) and a Polynomial Model (Degree 2).\n",
        "\n",
        "\n",
        "Imagine you are hiring a translator. You give them a test text (Training Data). They translate it perfectly. But did they memorize the dictionary, or do they know the language?\n",
        "To test this, you define 5 different texts.\n",
        "- You let them study 4, and test them on the 5th (Validation).\n",
        "- You repeat this for all combinations.\n",
        "\n",
        "If they consistently perform well on the unseen text, they are a good model."
      ],
      "metadata": {
        "id": "3Zg9rz_iuNGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "# 1. Generate Non-Linear Data (Parabola)\n",
        "# y = 0.5 * x^2 + noise\n",
        "X = np.random.rand(20, 1) * 10 - 5 # Random x between -5 and 5\n",
        "y = 0.5 * X**2 + np.random.randn(20, 1) # Quadratic relation\n",
        "\n",
        "# 2. Define K-Fold Splitter (K=4)\n",
        "kf = KFold(n_splits=4)\n",
        "\n",
        "linear_errors = []\n",
        "poly_errors = []\n",
        "\n",
        "# 3. Cross-Validation Loop\n",
        "for train_index, val_index in kf.split(X):\n",
        "    # Split data\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Model A: Linear\n",
        "    model_lin = LinearRegression()\n",
        "    model_lin.fit(X_train, y_train)\n",
        "    pred_lin = model_lin.predict(X_val)\n",
        "    error_lin = np.mean((y_val - pred_lin)**2)\n",
        "    linear_errors.append(error_lin)\n",
        "\n",
        "    # Model B: Quadratic (Polynomial Degree 2)\n",
        "    model_poly = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\n",
        "    model_poly.fit(X_train, y_train)\n",
        "    pred_poly = model_poly.predict(X_val)\n",
        "    error_poly = np.mean((y_val - pred_poly)**2)\n",
        "    poly_errors.append(error_poly)\n",
        "\n",
        "print(f\"Average Linear Error: {np.mean(linear_errors):.2f}\")\n",
        "print(f\"Average Poly Error:   {np.mean(poly_errors):.2f}\")\n",
        "# The Poly Error should be significantly lower."
      ],
      "metadata": {
        "id": "r_ztc1BTug9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task 7\n",
        "\n",
        "Perform a simple Train/Test split manually on the \"Car\" dataset (from Part 1, assuming you have 4 cars).\n",
        "\n",
        "1. Split indices: Train on indices , Test on .\n",
        "\n",
        "2. Train a Linear Regression model on the Train set to predict 'Top Speed' from 'Horsepower'.\n",
        "\n",
        "3. Predict on the Test set.\n",
        "\n",
        "4. Calculate and print the MSE."
      ],
      "metadata": {
        "id": "ZkpKsN_9ukLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#write code here"
      ],
      "metadata": {
        "id": "0ebRiOUFuxJv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}