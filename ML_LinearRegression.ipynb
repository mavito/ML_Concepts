{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn9/D69trtOUekimYLPs2d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavito/ML_Concepts/blob/main/ML_LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression in Machine Learning**\n",
        "\n",
        "\n",
        "The linear model is formally defined as an affine transformation mapping an input vector $\\mathbf{x} \\in \\mathbb{R}^D$ to a scalar output $y \\in \\mathbb{R}$. The model is parameterized by a weight vector $\\boldsymbol{\\theta}$ (often denoted as $\\mathbf{w}$) and an intercept (bias) $\\theta_0$.$$f(\\mathbf{x}) = \\boldsymbol{\\theta}^\\top \\mathbf{x} + \\theta_0$$"
      ],
      "metadata": {
        "id": "obDZgy9ku4hS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 Representation of Data as Matrices**\n",
        "\n",
        "In machine learning, we rarely process data points one by one. Instead, we stack them into matrices to utilize efficient linear algebra operations. If we have $N$ data points, each with $D$ features, we represent the entire dataset as a Design Matrix $\\mathbf{X}$ of shape $N \\times D$, and the targets as a vector $\\mathbf{y}$ of shape $N$.\n",
        "\n",
        "Imagine you have a spreadsheet of used cars. Each row is a single car. Each column is a feature, like \"horsepower,\" \"weight,\" or \"age.\" In math, we call this whole spreadsheet a Matrix (specifically, the Design Matrix). The price of each car is what we want to predict, so we rip that column out and call it the Target Vector.To make the math work for the \"base price\" (the y-intercept), we have to add a \"ghost column\" of 1s to the start of our spreadsheet. This is like saying every car has 1 unit of \"existence\" that contributes to the base price."
      ],
      "metadata": {
        "id": "af-au5oIYMR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# We will generate a synthetic dataset representing Car Prices\n",
        "# Features: [Horsepower, Age]\n",
        "# Target: Price\n",
        "\n",
        "def generate_car_data(n_samples=5):\n",
        "    # Random horsepower between 100 and 300\n",
        "    horsepower = np.random.randint(100, 300, n_samples)\n",
        "    # Random age between 1 and 10 years\n",
        "    age = np.random.randint(1, 10, n_samples)\n",
        "\n",
        "    # True relationship: Price = 100 * HP - 1000 * Age + 5000 (Base)\n",
        "    # We add some random noise because real life isn't perfect\n",
        "    noise = np.random.normal(0, 500, n_samples)\n",
        "    price = 100 * horsepower - 1000 * age + 5000 + noise\n",
        "\n",
        "    # Stack features into a matrix (N x 2)\n",
        "    X_raw = np.column_stack((horsepower, age))\n",
        "\n",
        "    return X_raw, price\n",
        "\n",
        "# 1. Generate the data\n",
        "X_raw, y = generate_car_data()\n",
        "\n",
        "print(\"Original Feature Matrix (Rows=Cars, Cols=Features):\")\n",
        "print(X_raw)\n",
        "\n",
        "# 2. Augment with the 'Ghost Column' of 1s for the bias term\n",
        "# We create a column of ones with the same number of rows as X\n",
        "ones_column = np.ones((X_raw.shape[0], 1))\n",
        "\n",
        "# We stick this column to the left side of our features\n",
        "X_design = np.hstack((ones_column, X_raw))\n",
        "\n",
        "print(\"\\nDesign Matrix (with bias column):\")\n",
        "print(X_design)\n",
        "print(\"\\nTarget Vector (Prices):\")\n",
        "print(y)"
      ],
      "metadata": {
        "id": "ioDDlc_Xzrg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.1:\n",
        "\n",
        "1. Load the Auto MPG dataset (a real-world dataset) using pandas.\n",
        "2. Select 'displacement' and 'horsepower' as your features and 'mpg' as your target.\n",
        "3. Convert these to numpy arrays.\n",
        "4. Create the Design Matrix $\\mathbf{X}$ by adding the column of ones to the features.\n",
        "5. Print the shape of your Design Matrix; it should be $(N, 3)$."
      ],
      "metadata": {
        "id": "6GSVaqVp0PWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "73VedPQskVWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2. Least Squares and Orthogonal Projection**\n",
        "\n",
        "\n",
        "The second conceptual hurdle is defining \"best fit.\" The geometric perspective defines the best fit as the parameter vector $\\boldsymbol{\\theta}$ that minimizes the Euclidean distance between the target vector $\\mathbf{y}$ and the subspace spanned by the feature columns of $\\mathbf{X}$. This is the method of Ordinary Least Squares (OLS).The loss function is defined as the sum of squared residuals:$$J(\\boldsymbol{\\theta}) = \\frac{1}{2} ||\\mathbf{X}\\boldsymbol{\\theta} - \\mathbf{y}||^2$$Minimizing this quadratic function yields the famous Normal Equations:$$\\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\theta} = \\mathbf{X}^\\top \\mathbf{y}$$"
      ],
      "metadata": {
        "id": "sN_8X1Sj0Qe8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine you are standing in a room (the space of your data).\n",
        "\n",
        "The target value $\\mathbf{y}$ is a point hanging in the air. Your features define the floor of the room.\n",
        "\n",
        "You want to find the point on the floor that is directly underneath the hanging point $\\mathbf{y}$. This is the \"shadow\" of $\\mathbf{y}$ on the floor.\n",
        "\n",
        "The math formula $\\boldsymbol{\\theta} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{y}$ is the instruction manual for calculating exactly where that shadow falls.\n",
        "\n",
        "It tells you exactly how much of each feature to mix together to get as close as possible to the target."
      ],
      "metadata": {
        "id": "y6DjLiih5h6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_ols_weights(X, y):\n",
        "    \"\"\"\n",
        "    Computes the optimal weights using the Normal Equation.\n",
        "    Formula: theta = (X^T * X)^-1 * X^T * y\n",
        "    \"\"\"\n",
        "    # 1. Compute X Transpose (X^T)\n",
        "    X_T = X.T\n",
        "\n",
        "    # 2. Compute the Gram Matrix (X^T * X)\n",
        "    # This matrix captures how features are correlated with each other\n",
        "    gram_matrix = np.dot(X_T, X)\n",
        "\n",
        "    # 3. Compute the inverse of the Gram Matrix\n",
        "    # Note: In real production code, we use np.linalg.solve for stability\n",
        "    # but here we use inv to explicitly show the formula.\n",
        "    gram_inv = np.linalg.inv(gram_matrix)\n",
        "\n",
        "    # 4. Compute X^T * y (Correlation between features and target)\n",
        "    moment_vector = np.dot(X_T, y)\n",
        "\n",
        "    # 5. Multiply inverse with moment vector to get weights\n",
        "    theta = np.dot(gram_inv, moment_vector)\n",
        "\n",
        "    return theta\n",
        "\n",
        "# Using the car data from the previous step\n",
        "theta_estimated = compute_ols_weights(X_design, y)\n",
        "\n",
        "print(\"Estimated Weights:\")\n",
        "print(f\"Base Price (Bias): {theta_estimated[0]}\")\n",
        "print(f\"Price per Horsepower: {theta_estimated[1]}\")\n",
        "print(f\"Price per Year of Age: {theta_estimated[2]}\")\n",
        "\n",
        "# Making a prediction for a new car\n",
        "# HP=200, Age=5\n",
        "new_car = np.array([1,200,5]) # Don't forget the bias 1!\n",
        "predicted_price = np.dot(theta_estimated, new_car)\n",
        "print(f\"\\nPredicted price for 200HP, 5yo car: ${predicted_price}\")"
      ],
      "metadata": {
        "id": "i9BYj66PNMNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.2\n",
        "\n",
        "- Using the auto_mpg design matrix $\\mathbf{X}$ and target $\\mathbf{y}$ you created in the previous task\n",
        "- Implement the compute_ols_weights function.\n",
        "- Calculate the weights for predicting MPG.\n",
        "- Interpret the weights: Does higher horsepower increase or decrease the MPG based on your model? (Hint: Check the sign of the weight associated with the horsepower column)."
      ],
      "metadata": {
        "id": "Q3ZFEP79NPqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Probabilistic Modeling and Maximum Likelihood Estimation (MLE)**\n",
        "While the geometric view provides the mechanics of regression, the probabilistic view provides the justification for why we minimize squared error. This module introduces Maximum Likelihood Estimation (MLE), a cornerstone of statistical machine learning."
      ],
      "metadata": {
        "id": "YMqmHJpQ69zH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maximum Likelihood Estimation (MLE) is a detective technique.\n",
        "\n",
        "We ask: \"Which line makes the data we actually observed the most probable?\"\n",
        "\n",
        "If we assume the data points are scattered around the true line like a bell curve (Gaussian distribution), we can calculate a \"likelihood score\" for any candidate line. The best line is the one with the highest score.\n",
        "\n",
        "-----\n",
        "\n",
        "Imagine you are throwing darts at a line drawn on the floor. You are a good player, so your darts usually land close to the line, and rarely land far away. This spread follows a bell curve.\n",
        "\n",
        "Now, reverse the situation.\n",
        "\n",
        "You see a bunch of darts sticking in the floor, but the line has been erased. You have to draw the line back in. Where do you put it? You draw it right through the middle of the darts. Why? Because that position makes it most likely that the darts landed where they did. If you drew the line far away, it would be extremely unlikely (low probability) that you threw the darts so badly they landed over here.\n",
        "\n",
        "MLE is just the math for \"drawing the line through the middle of the mess.\""
      ],
      "metadata": {
        "id": "hp6tdKMe8uVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "\n",
        "def plot_gaussian_residuals(X, y, theta, sigma=500):\n",
        "    \"\"\"\n",
        "    Visualizes the Gaussian probability of data points given a line.\n",
        "    \"\"\"\n",
        "    # Predict values\n",
        "    y_pred = X.dot(theta)\n",
        "\n",
        "    # Calculate residuals (distance from line)\n",
        "    residuals = y - y_pred\n",
        "\n",
        "    # Calculate Likelihood score (ignoring constants for simplicity)\n",
        "    # We sum the log-probabilities\n",
        "    log_likelihood = np.sum(norm.logpdf(residuals, loc=0, scale=sigma))\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(X[:, 1], y, color='blue', label='Data Points') # X[:,1] is raw feature\n",
        "    plt.plot(X[:, 1], y_pred, color='red', label='Model Line')\n",
        "\n",
        "    # Draw lines showing residuals\n",
        "    for i in range(len(X)):\n",
        "        plt.plot([X[i, 1], X[i, 1]], [y[i], y_pred[i]], 'g--', alpha=0.5)\n",
        "\n",
        "    plt.title(f\"Linear Fit with Residuals\\nLog-Likelihood Score: {log_likelihood:.2f}\")\n",
        "    plt.xlabel(\"Feature\")\n",
        "    plt.ylabel(\"Target\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Let's see how a 'bad' guess looks vs a 'good' guess\n",
        "# Good guess (from OLS)\n",
        "plot_gaussian_residuals(X_design, y, theta_estimated)\n",
        "\n",
        "# Bad guess (random weights)\n",
        "theta_bad = np.array([0,0,0])\n",
        "plot_gaussian_residuals(X_design, y, theta_bad)"
      ],
      "metadata": {
        "id": "wdmEXJVbk4IX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 :\n",
        "\n",
        "- Using the auto_mpg data.\n",
        "\n",
        "- Define a \"bad\" set of weights (e.g., set them all to zero).\n",
        "\n",
        "- Calculate the Log-Likelihood of the bad weights.\n",
        "\n",
        "- Calculate the Log-Likelihood of the OLS weights you found in Task 1.\n",
        "\n",
        "- Compare the numbers. Note that Log-Likelihoods are usually negative numbers. The value closer to 0 (less negative) is better. Confirm that the OLS weights have a higher likelihood than the zero weights."
      ],
      "metadata": {
        "id": "OYbMNCOv89YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "GK5_VhXx9vak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Model Complexity, Polynomials, and Regularization**"
      ],
      "metadata": {
        "id": "6F0V9e43_Cw-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.1 Polynomials and Overfitting**\n",
        "\n",
        "Sometimes data is curved, not straight. We can still use linear regression by \"cheating.\" We create new features that are powers of the original feature ($x^2, x^3$, etc.). This is called Polynomial Regression. Even though the curve looks bendy, the math is still linear because we are just multiplying weights by these new numbers.\n",
        "\n",
        "---\n",
        "Imagine trying to fit a piece of rigid wood (a straight line) to a curved banana. It won't work. Now imagine you have a flexible ruler. By bending it (adding $x^2, x^3$), you can match the curve of the banana.However, if you make the ruler too flexible (like a piece of string), and you try to touch every single spot on a bumpy banana, the string will wiggle and loop wildly between the bumps. This is **Overfitting**. The string touches every data point, but it looks like a mess and doesn't represent the general shape of the banana anymore."
      ],
      "metadata": {
        "id": "DP4qdPANiqul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "def demo_polynomial_overfitting(n_points=10):\n",
        "    # Generate noisy sine wave data\n",
        "    rng = np.random.RandomState(1)\n",
        "    x = 10 * rng.rand(n_points)\n",
        "    y = np.sin(x) + 0.1 * rng.randn(n_points)\n",
        "\n",
        "    # Create a range of x values for smooth plotting\n",
        "    x_plot = np.linspace(0, 10, 100)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Try different degrees\n",
        "    degrees = [1,2,4]\n",
        "    colors = ['green', 'blue', 'red']\n",
        "\n",
        "    plt.scatter(x, y, color='black', label='Data')\n",
        "\n",
        "    for i, degree in enumerate(degrees):\n",
        "        # Create a pipeline:\n",
        "        # 1. Turn x into [1, x, x^2,..., x^degree]\n",
        "        # 2. Run Linear Regression\n",
        "        model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "        model.fit(x[:, np.newaxis], y)\n",
        "        y_plot = model.predict(x_plot[:, np.newaxis])\n",
        "\n",
        "        plt.plot(x_plot, y_plot, color=colors[i],\n",
        "                 label=f'Degree {degree}')\n",
        "\n",
        "    plt.ylim(-2, 2)\n",
        "    plt.legend()\n",
        "    plt.title(\"Polynomial Regression: Underfitting vs Good Fit vs Overfitting\")\n",
        "    plt.show()\n",
        "\n",
        "demo_polynomial_overfitting()"
      ],
      "metadata": {
        "id": "RQpJ1Zr-jD5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2 Regularization (Ridge Regression)**\n",
        "\n",
        "To stop the \"string\" from wiggling too much, we use Regularization. We add a penalty to the loss function that punishes the model for having very large weights. Large weights usually cause the wild oscillations seen in overfitting. This is called Ridge Regression (or L2 Regularization).\n",
        "\n",
        "---\n",
        "Think of the weights $\\boldsymbol{\\theta}$ as the \"energy\" or \"tension\" in the string. To make the string curve wildly to hit every point, you have to pull it very tight and twist it hard (use large weights). Regularization is like adding a tax on tension. You tell the model: \"You can fit the data, but I will charge you a fee for every bit of tension you put in the string.\" The model naturally becomes lazy; it tries to fit the data reasonably well, but it refuses to twist wildly because that would be too expensive (high tax). The result is a smoother curve."
      ],
      "metadata": {
        "id": "Vl5AeGW5-7z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "def demo_ridge_regularization(n_points=10):\n",
        "    # Same data\n",
        "    rng = np.random.RandomState(1)\n",
        "    x = 10 * rng.rand(n_points)\n",
        "    y = np.sin(x) + 0.1 * rng.randn(n_points)\n",
        "    x_plot = np.linspace(0, 10, 100)\n",
        "\n",
        "    # Use a high degree that normally overfits\n",
        "    degree = 10\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(x, y, color='black', label='Data')\n",
        "\n",
        "    # 1. Unregularized (Standard Linear Regression)\n",
        "    # This is like Ridge with alpha=0\n",
        "    model_wild = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
        "    model_wild.fit(x[:, np.newaxis], y)\n",
        "    y_wild = model_wild.predict(x_plot[:, np.newaxis])\n",
        "    plt.plot(x_plot, y_wild, 'r--', label=f'No Regularization (Deg {degree})')\n",
        "\n",
        "    # 2. Regularized (Ridge)\n",
        "    # alpha is the \"tax rate\" (lambda)\n",
        "    model_tame = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=0.1))\n",
        "    model_tame.fit(x[:, np.newaxis], y)\n",
        "    y_tame = model_tame.predict(x_plot[:, np.newaxis])\n",
        "    plt.plot(x_plot, y_tame, 'g-', label=f'Ridge Regularization (Deg {degree})')\n",
        "\n",
        "    plt.ylim(-2, 2)\n",
        "    plt.legend()\n",
        "    plt.title(\"Taming Overfitting with Ridge Regularization\")\n",
        "    plt.show()\n",
        "\n",
        "demo_ridge_regularization()"
      ],
      "metadata": {
        "id": "CGYvNg7G_nQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3\n",
        "\n",
        "1. Use the generate_car_data function but reduce the number of samples to just 5.\n",
        "\n",
        "2. Fit a Polynomial Regression model of Degree 4 (which is too complex for 5 points).\n",
        "\n",
        "3. Print the coefficients (weights) of this model. Notice how huge they are (e.g., millions or billions).\n",
        "\n",
        "4. Fit a Ridge Regression model (Degree 4) with alpha=10.\n",
        "\n",
        "5. Print the coefficients of the Ridge model. Notice how much smaller they are.\n",
        "\n",
        "Explain in a markdown cell: Why does the unregularized model have such huge weights?."
      ],
      "metadata": {
        "id": "7TnFl0zFA1Gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Bayesian Linear Regression**\n",
        "\n",
        "Standard Linear Regression gives you one single line. But if you have very little data, are you sure that one line is the correct one? Probably not.\n",
        "\n",
        "Bayesian Regression doesn't give you a single line; it gives you a \"cloud\" of possible lines. It tells you: \"I'm pretty sure the line is somewhere in this region.\"\n",
        "Where there is lots of data, the cloud is tight (high confidence). Where there is no data, the cloud puffs out (low confidence).\n",
        "\n",
        "---\n",
        "Imagine you are holding a handful of spaghetti strands. You throw them on a table and try to arrange them so they all pass through two specific points (data). You can wiggle the spaghetti strands a bit; many different angles will still touch those two points.\n",
        "\n",
        "Standard regression picks the one strand of spaghetti that fits best.\n",
        "\n",
        "Bayesian regression keeps all the strands that fit reasonably well. When you ask for a prediction, Bayesian regression looks at where all the strands go. If all the strands go to the same place, the model says \"I'm confident.\" If the strands splay out in different directions, the model says \"I don't know, it could be any of these.\""
      ],
      "metadata": {
        "id": "IDrGBHgD97rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayesian_inference_demo():\n",
        "    # True function\n",
        "    def f(x, noise_amount):\n",
        "        y = -0.3 + 0.5 * x\n",
        "        noise = np.random.normal(0, noise_amount, len(x))\n",
        "        return y + noise\n",
        "\n",
        "    # Generate data\n",
        "    X = np.array([-1, 1]) # Only 2 data points!\n",
        "    y = f(X, noise_amount=0.1)\n",
        "\n",
        "    # Add bias term\n",
        "    X_design = np.column_stack((np.ones(len(X)), X))\n",
        "\n",
        "    # Hyperparameters\n",
        "    alpha = 2.0  # Prior precision (how sure we are that weights are small)\n",
        "    beta = 25.0  # Noise precision (how sure we are about the data)\n",
        "\n",
        "    # Calculate Posterior Covariance (S_N) and Mean (m_N)\n",
        "    # Formula: S_N = inv(alpha*I + beta*X.T*X)\n",
        "    S_N_inv = alpha * np.eye(2) + beta * np.dot(X_design.T, X_design)\n",
        "    S_N = np.linalg.inv(S_N_inv)\n",
        "\n",
        "    # Formula: m_N = beta * S_N * X.T * y\n",
        "    m_N = beta * np.dot(S_N, np.dot(X_design.T, y))\n",
        "\n",
        "    # Visualize the \"Cloud of Lines\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # 1. Plot the data points\n",
        "    plt.scatter(X, y, color='red', s=100, zorder=10, label='Data')\n",
        "\n",
        "    # 2. Sample 20 possible lines from the posterior distribution\n",
        "    # We treat the weights as random variables\n",
        "    possible_weights = np.random.multivariate_normal(m_N, S_N, 20)\n",
        "\n",
        "    x_range = np.linspace(-2, 2, 100)\n",
        "    x_range_design = np.column_stack((np.ones(100), x_range))\n",
        "\n",
        "    for i, w in enumerate(possible_weights):\n",
        "        y_sample = np.dot(x_range_design, w)\n",
        "        plt.plot(x_range, y_sample, 'b-', alpha=0.3)\n",
        "\n",
        "    plt.title(\"Bayesian Linear Regression: The Cloud of Possibility\")\n",
        "    plt.xlabel(\"Input\")\n",
        "    plt.ylabel(\"Output\")\n",
        "    plt.show()\n",
        "\n",
        "bayesian_inference_demo()"
      ],
      "metadata": {
        "id": "UPoLS7av90pU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4\n",
        "\n",
        "1. Run the bayesian_inference_demo.\n",
        "\n",
        "2. Change X to include more points (e.g., np.linspace(-1, 1, 10)). Rerun and observe what happens to the \"cloud\" of lines. (Answer: It should get tighter/narrower).\n",
        "\n",
        "3. Change beta to a small number (e.g., 1.0). This means we assume the data is very noisy. Observe the result. (Answer: The cloud should get wider, as we trust the data less)."
      ],
      "metadata": {
        "id": "LyOYulzo-YT1"
      }
    }
  ]
}