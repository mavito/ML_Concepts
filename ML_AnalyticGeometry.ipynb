{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+BdmT70Nyi4CI21MnJb+s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mavito/ML_Concepts/blob/main/ML_AnalyticGeometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analytic Geometry in Machine Learning**\n",
        "\n",
        "- ## Assignment Architecture and Mini-Dataset:\n",
        "\n",
        "To facilitate the practical exploration of these concepts, we define a \"mini-dataset\" of geometric vectors.\n",
        "\n",
        "Unlike the high-dimensional sparse datasets often used in production environments, we restrict our initial exploration to $\\mathbb{R}^2$ and $\\mathbb{R}^3$.\n",
        "\n",
        "This intentional low-dimensionality allows for direct visualization and mental modeling, which is crucial for grasping concepts like projections and rotations before scaling them to $\\mathbb{R}^{1000}$."
      ],
      "metadata": {
        "id": "obDZgy9ku4hS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GLOBAL SETUP FOR THE ASSIGNMENT\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ERx-ahxF0L0Z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini-Dataset: Geometric Vectors\n",
        "# We define a small set of vectors in R^2 and R^3 to use across all exercises.\n",
        "# These represent data points (features) in a geometric space.\n",
        "\n",
        "# Vectors in 2D (e.g., Feature 1 vs Feature 2)\n",
        "# vec_a represents a standard data point in quadrant I\n",
        "vec_a = np.array([3,4])\n",
        "# vec_b is close to the origin, representing a smaller magnitude signal\n",
        "vec_b = np.array([1,2])\n",
        "# vec_c is in quadrant II, useful for testing angles > 90 degrees\n",
        "vec_c = np.array([-2, 1])"
      ],
      "metadata": {
        "id": "ioDDlc_Xzrg9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectors in 3D (e.g., Feature 1, Feature 2, Feature 3)\n",
        "# These allow us to test cross-products and 3D subspaces\n",
        "vec_x = np.array([1, 0, 0])\n",
        "vec_y = np.array([0, 1, 0])\n",
        "vec_z = np.array([1, -1, 0])\n",
        "print(\"Dataset Initialized.\")\n",
        "print(f\"2D Vectors: a={vec_a}, b={vec_b}, c={vec_c}\")\n",
        "print(f\"3D Vectors: x={vec_x}, y={vec_y}, z={vec_z}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GSVaqVp0PWn",
        "outputId": "8b62ce4b-5dac-4501-e79c-2ae9de1fbed6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Initialized.\n",
            "2D Vectors: a=[3 4], b=[1 2], c=[-2  1]\n",
            "3D Vectors: x=[1 0 0], y=[0 1 0], z=[ 1 -1  0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Norms: The Measurement of Magnitude and Sparsity**\n",
        "\n",
        "Think of a norm as a \"ruler\" for vectors.\n",
        "\n",
        "Just as a physical ruler measures how long a piece of string is, a mathematical norm measures how \"long\" a vector is or how far it is from the origin (zero).\n",
        "\n",
        "However, in the world of data, there are different \"rules of travel\" that change how we measure length.\n",
        "\n",
        "- **Euclidean Norm ($L_2$):**\n",
        "  - Imagine you are a bird flying directly from the start to the end of the vector.\n",
        "  - You fly in a straight line, ignoring buildings or obstacles.\n",
        "  - The distance you cover is the Euclidean norm.\n",
        "  - This is the standard \"straight-line\" distance we learn in school ($a^2 + b^2 = c^2$).\n",
        "\n",
        "- **Manhattan Norm ($L_1$):**\n",
        "  - Imagine you are a taxi driver in a city like New York with grid-like streets.\n",
        "  - You cannot drive through buildings (diagonally); you must stick to the streets.\n",
        "  - To get to your destination, you drive the distance east, then the distance north.\n",
        "  - The total distance on your odometer is the Manhattan norm.\n",
        "  - It is simply the sum of the absolute blocks you traveled.\n",
        "\n",
        "*The following code block demonstrates the calculation of these norms from first principles (manual calculation) and verifies them against optimized libraries.*"
      ],
      "metadata": {
        "id": "sN_8X1Sj0Qe8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_norms(vector):\n",
        "    \"\"\"\n",
        "    Calculates L1 and L2 norms of a vector manually and using numpy.\n",
        "\n",
        "    Args:\n",
        "    vector (np.array): Input vector\n",
        "\n",
        "    Returns:\n",
        "    tuple: (l1_manual, l2_manual, l1_numpy, l2_numpy)\n",
        "    \"\"\"\n",
        "    # 1. L1 Norm (Manhattan): Sum of absolute values\n",
        "    # Intuition: Taxi driving distance along grid lines\n",
        "    # Math: |x_1| + |x_2| +...\n",
        "    l1_manual = np.sum(np.abs(vector))\n",
        "\n",
        "    # 2. L2 Norm (Euclidean): Square root of sum of squares\n",
        "    # Intuition: Straight line distance (hypotenuse)\n",
        "    # Math: sqrt(x_1^2 + x_2^2 +...)\n",
        "    l2_manual = np.sqrt(np.sum(vector**2))\n",
        "\n",
        "    # Verification using numpy's linear algebra module\n",
        "    # The 'ord' parameter specifies the order of the norm\n",
        "    l1_numpy = np.linalg.norm(vector, 1)\n",
        "    l2_numpy = np.linalg.norm(vector, 2)\n",
        "\n",
        "    return l1_manual, l2_manual, l1_numpy, l2_numpy\n",
        "\n",
        "# Applying to our dataset vector 'vec_a' which is\n",
        "l1_a, l2_a, _, _ = calculate_norms(vec_a)\n",
        "\n",
        "print(f\"Vector a: {vec_a}\")\n",
        "print(f\"L1 Norm (Taxi dist): {l1_a} (Calculation: |3| + |4|)\")\n",
        "print(f\"L2 Norm (Bird dist): {l2_a} (Calculation: sqrt(3^2 + 4^2) = sqrt(9+16) = 5)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdst_KHC5fCX",
        "outputId": "d78abc86-c264-48c7-e10e-e22cd541f486"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector a: [3 4]\n",
            "L1 Norm (Taxi dist): 7 (Calculation: |3| + |4|)\n",
            "L2 Norm (Bird dist): 5.0 (Calculation: sqrt(3^2 + 4^2) = sqrt(9+16) = 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1.1 : Calculate the $L_{\\infty}$ (Infinity) norm for vec_c and vec_x.\n",
        "\n",
        "*Context:*\n",
        "\n",
        "The Infinity norm answers the question: \"What is the single largest component of this vector?\"\n",
        "\n",
        "It represents the maximum dominance of any single feature.\n",
        "\n",
        "In geometry, the unit ball of the $L_{\\infty}$ norm is a square (in 2D) or a cube (in 3D).\n",
        "\n",
        "Math: $\\|x\\|_{\\infty} = \\max(|x_1|, |x_2|,..., |x_n|)$.\n",
        "\n",
        "*Problem:*\n",
        "\n",
        "Implement a function calculate_inf_norm(vector) that finds this value using standard python logic (no np.linalg.norm initially), then verify your result with np.linalg.norm(vector, np.inf).\n"
      ],
      "metadata": {
        "id": "siSI_7Jk53FP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# WRITE CODE HERE"
      ],
      "metadata": {
        "id": "e8tArvvl6zPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Inner Products**\n",
        "\n",
        "The inner product (commonly called the dot product) is a mathematical way to calculate how much two vectors \"agree\" or point in the same direction. It is the measure of their alignment.\n",
        "\n",
        "Imagine two arrows starting from the same spot.\n",
        "\n",
        "- **High Positive Value:**\n",
        "  - If they point in roughly the same direction, their inner product is positive and large.\n",
        "  - They have high \"agreement.\"\n",
        "\n",
        "- **Zero:**\n",
        "  - If they point in perpendicular directions (like the corner of a room, 90 degrees), their inner product is zero.\n",
        "  - They have \"zero agreement\"—changing one vector doesn't affect the direction of the other at all.\n",
        "  - They are independent.\n",
        "\n",
        "- **Negative Value:**\n",
        "  - If they point in opposite directions (greater than 90 degrees), the inner product is negative.\n",
        "  - They disagree.\n",
        "\n",
        "In machine learning recommendation systems, we use this to check similarity.\n",
        "If a \"user preference\" vector has a high inner product with a \"movie feature\" vector, it implies the user's interests align with the movie's content, predicting a \"watch.\""
      ],
      "metadata": {
        "id": "YMqmHJpQ69zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python Implementation : INNER PRODUCTS\n",
        "\n",
        "def compute_inner_product(v1, v2):\n",
        "    \"\"\"\n",
        "    Computes the dot product (standard inner product) of two vectors.\n",
        "    \"\"\"\n",
        "    # Method 1: Element-wise multiplication and sum\n",
        "    # Math: x_1*y_1 + x_2*y_2 +...\n",
        "    # This explicit calculation highlights the 'sum of interactions'\n",
        "    dot_manual = np.sum(v1 * v2)\n",
        "\n",
        "    # Method 2: Numpy dot function\n",
        "    # This is the optimized implementation used in production\n",
        "    dot_numpy = np.dot(v1, v2)\n",
        "\n",
        "    return dot_manual, dot_numpy\n",
        "\n",
        "# Analyzing relationship between vec_a (3,4) and vec_b (1,2)\n",
        "dot_ab, _ = compute_inner_product(vec_a, vec_b)\n",
        "\n",
        "# Analyzing relationship between vec_a (3,4) and a perpendicular vector (-4, 3)\n",
        "# We define a vector visually perpendicular to (3,4) to test the zero property.\n",
        "vec_perp = np.array([-4, 3])\n",
        "dot_perp, _ = compute_inner_product(vec_a, vec_perp)\n",
        "\n",
        "print(f\"Inner Product (a, b): {dot_ab} (Positive -> roughly same direction)\")\n",
        "print(f\"Inner Product (a, perp): {dot_perp} (Zero -> Perpendicular/Orthogonal)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp6tdKMe8uVc",
        "outputId": "8960e42a-b559-4218-b5e5-7be0763bdd0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inner Product (a, b): 11 (Positive -> roughly same direction)\n",
            "Inner Product (a, perp): 0 (Zero -> Perpendicular/Orthogonal)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2.1 : Define a \"weighted\" inner product.\n",
        "\n",
        "*Context:*\n",
        "\n",
        "In many ML problems, certain features (dimensions) are more reliable or important than others.\n",
        "\n",
        "A standard dot product treats all dimensions equally.\n",
        "\n",
        "A weighted inner product allows us to emphasize certain dimensions by multiplying the components by weights before summing.\n",
        "\n",
        "This corresponds to the form $\\langle x, y \\rangle_W = x^T W y$, where $W$ is a diagonal matrix of weights.\n",
        "\n",
        "*Problem:*\n",
        "\n",
        "Calculate the weighted inner product of vec_x and vec_y where the weight matrix is $W = \\text{diag}(2, 1, 5)$.\n",
        "\n",
        "This means the interaction in the first dimension is worth 2x, the second is 1x, and the third is 5x.\n",
        "\n",
        "Formula to implement: $\\langle x, y \\rangle_W = 2x_1y_1 + 1x_2y_2 + 5x_3y_3$."
      ],
      "metadata": {
        "id": "OYbMNCOv89YF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "GK5_VhXx9vak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Lengths and Distances**\n",
        "\n",
        "Distance is simply the length of the line connecting two data points.\n",
        "\n",
        "- **Difference:**\n",
        "  - If you have two data points, $A$ and $B$, you first find the \"difference vector\" ($B - A$).\n",
        "  - This vector represents the path or displacement to get from $A$ to $B$\n",
        "  \n",
        "- **Measurement:**\n",
        "  - You then measure the length (norm) of that difference vector using your chosen ruler (Euclidean or Manhattan).\n",
        "  \n",
        "In a machine learning context, if point $A$ represents the data for a \"dog\" image and point $B$ represents a \"cat\" image:\n",
        "\n",
        "- A small distance means the computer thinks the images look very similar (perhaps both are furry animals).\n",
        "- A large distance means they look very different.\n",
        "\n",
        "Algorithms like K-Means clustering use this concept to group similar data points together."
      ],
      "metadata": {
        "id": "IDrGBHgD97rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-TOPIC: LENGTHS AND DISTANCES\n",
        "\n",
        "def euclidean_distance(v1, v2):\n",
        "    \"\"\"\n",
        "    Computes the Euclidean distance between two vectors.\n",
        "    \"\"\"\n",
        "    # Step 1: Find the difference vector (displacement)\n",
        "    # This vector represents the path from v2 to v1\n",
        "    diff = v1 - v2\n",
        "\n",
        "    # Step 2: Measure the length (norm) of the difference\n",
        "    # We use L2 norm for standard Euclidean distance\n",
        "    distance = np.linalg.norm(diff)\n",
        "\n",
        "    return distance\n",
        "\n",
        "# Distance between vec_a (3,4) and vec_b (1,2)\n",
        "dist_ab = euclidean_distance(vec_a, vec_b)\n",
        "\n",
        "# Distance between vec_a and vec_c (-2, 1)\n",
        "dist_ac = euclidean_distance(vec_a, vec_c)\n",
        "\n",
        "print(f\"Distance a <-> b: {dist_ab:.2f}\")\n",
        "print(f\"Distance a <-> c: {dist_ac:.2f}\")\n",
        "print(\"Interpretation: Since dist_ab < dist_ac, vector 'a' is geometrically closer to 'b'.\")"
      ],
      "metadata": {
        "id": "karK3Y73-gO1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eef4eb8-1df1-4f68-a661-1aa081b212e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distance a <-> b: 2.83\n",
            "Distance a <-> c: 5.83\n",
            "Interpretation: Since dist_ab < dist_ac, vector 'a' is geometrically closer to 'b'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.1 : Calculate the distance between vec_x and vec_z using the Manhattan ($L_1$) metric.\n",
        "\n",
        "**Instruction:**\n",
        "\n",
        "Remember that Manhattan distance is the sum of the absolute differences of the coordinates.\n",
        "\n",
        "It asks: \"How far do I have to travel if I can only change one feature at a time?\"\n",
        "\n",
        "**Formula:**\n",
        "\n",
        "$d_1(x, z) = \\sum |x_i - z_i|$.\n",
        "\n",
        "**Analysis:**\n",
        "\n",
        "*Is the Manhattan distance larger or smaller than the Euclidean distance for these two vectors? (Verify this with your code).*\n",
        "\n",
        "*Hint: The \"Taxi\" route is usually longer than the \"Bird\" route.*"
      ],
      "metadata": {
        "id": "msv-497s-s5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "bxlzzkks_GQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Angles and Orthogonality**\n",
        "\n",
        "The angle tells us about the relationship or quality of the connection between two features, completely ignoring how \"strong\" or \"loud\" those features are (their size).\n",
        "\n",
        "- Angle = 0 degrees:\n",
        "  - The vectors are \"parallel.\"\n",
        "  - They point in the same direction.\n",
        "  - When one increases, the other increases.\n",
        "  - This implies a Perfect Positive Correlation.\n",
        "\n",
        "- Angle = 90 degrees:\n",
        "  - The vectors are \"orthogonal.\"\n",
        "  - Knowing the value of one tells you absolutely nothing about the value of the other.\n",
        "  - They are Uncorrelated or Independent.\n",
        "\n",
        "- Angle = 180 degrees:\n",
        "  - They are opposites.\n",
        "  - When one goes up, the other goes down.\n",
        "  - This implies a Negative Correlation.\n",
        "\n"
      ],
      "metadata": {
        "id": "wKKW0QLU_JMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-TOPIC: ANGLES AND ORTHOGONALITY\n",
        "def angle_between(v1, v2):\n",
        "    \"\"\"\n",
        "    Computes the angle in degrees between two vectors using the definition of dot product.\n",
        "    \"\"\"\n",
        "    # 1. Compute Inner Product (The interaction)\n",
        "    dot_prod = np.dot(v1, v2)\n",
        "    # 2. Compute Norms (The magnitudes)\n",
        "    norm_v1 = np.linalg.norm(v1)\n",
        "    norm_v2 = np.linalg.norm(v2)\n",
        "    # 3. Compute Cosine Similarity\n",
        "    # Formula: cos(theta) = (a. b) / (|a| * |b|)\n",
        "    # We clip values to [-1, 1] to handle potential floating point errors slightly outside range\n",
        "    cos_theta = np.clip(dot_prod / (norm_v1 * norm_v2), -1.0, 1.0)\n",
        "    # 4. Convert to Angle\n",
        "    # arccos returns radians, so we convert to degrees for readability\n",
        "    angle_rad = np.arccos(cos_theta)\n",
        "    angle_deg = np.degrees(angle_rad)\n",
        "    return cos_theta, angle_deg\n",
        "\n",
        "cos_sim, angle = angle_between(vec_a, vec_b)\n",
        "print(f\"Vector a: {vec_a}, Vector b: {vec_b}\")\n",
        "print(f\"Cosine Similarity: {cos_sim:.3f}\")\n",
        "print(f\"Angle: {angle:.2f} degrees\")\n",
        "\n",
        "# Check Orthogonality explicitly\n",
        "# Vectors [1,0] and [0,1] are the standard x and y axes\n",
        "v_ortho_1 = np.array([1, 0])\n",
        "v_ortho_2 = np.array([0, 1])\n",
        "_, angle_ortho = angle_between(v_ortho_1, v_ortho_2)\n",
        "print(f\"Angle between [1,0] and [0,1]: {angle_ortho} degrees (Expect 90.0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pAcTR7K_f-m",
        "outputId": "a695169a-da71-4cab-dc9c-a5aac5a0b8d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector a: [3 4], Vector b: [1 2]\n",
            "Cosine Similarity: 0.984\n",
            "Angle: 10.30 degrees\n",
            "Angle between [1,0] and [0,1]: 90.0 degrees (Expect 90.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4.1: Find a vector that is orthogonal to vec_x ($$).\n",
        "\n",
        "**Constraint:**\n",
        "\n",
        "The vector must be non-zero.\n",
        "\n",
        "**Method:**\n",
        "\n",
        "You need a vector $u = [u_1, u_2, u_3]$ such that the dot product is zero:\n",
        "\n",
        "$1\\cdot u_1 + 0\\cdot u_2 + 1\\cdot u_3 = 0$.\n",
        "\n",
        "This simplifies to $u_1 + u_3 = 0$.\n",
        "\n",
        "**Action:**\n",
        "\n",
        "*Choose values for $u_1, u_2, u_3$ that satisfy this equation (there are infinite solutions).*\n",
        "\n",
        "*Define this vector in Python and use the angle_between function to prove the angle is 90 degrees.*"
      ],
      "metadata": {
        "id": "InOZKkvO_qdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "f8LUCDMbAIg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Orthonormal Basis**\n",
        "\n",
        "Imagine you are tiling a floor. To do a good job, you need reference lines.\n",
        "\n",
        "- It is easiest if your reference lines are exactly 90 degrees apart (Orthogonal).\n",
        "  - If they are skewed, your tiles will require difficult cuts.\n",
        "\n",
        "- It is easiest if your measuring stick is exactly 1 meter long (Normalized).\n",
        "  - If your ruler is 1.34 meters long, the math for calculating area becomes messy.\n",
        "\n",
        "An Orthonormal Basis is just a \"clean\" coordinate system where every axis is perpendicular to the others and has a scale of 1.\n",
        "\n",
        "The Gram-Schmidt process is a mathematical recipe for taking a messy, skewed set of arrows and straightening them out into a clean, perpendicular grid without losing the space they cover.\n",
        "\n",
        "It's like tidying up the geometry."
      ],
      "metadata": {
        "id": "OlD6zRAsAJko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-TOPIC: ORTHONORMAL BASIS (Gram-Schmidt)\n",
        "def gram_schmidt(vectors):\n",
        "    \"\"\"\n",
        "    Applies Gram-Schmidt process to a list of vectors to create an Orthonormal Basis.\n",
        "    \"\"\"\n",
        "    basis = []\n",
        "    for v in vectors:\n",
        "        # 1. Orthogonalization step\n",
        "        # Start with the original vector v\n",
        "        w = v.copy()\n",
        "        # Subtract the projection of v onto every vector currently in our basis\n",
        "        # This removes the \"redundant\" direction that we have already captured\n",
        "        for b in basis:\n",
        "            projection = np.dot(v, b) * b\n",
        "            w = w - projection\n",
        "        # 2. Normalization step\n",
        "        # If the remaining vector w is not zero, scale it to length 1\n",
        "        if np.linalg.norm(w) > 1e-10:  # Check for near-zero to avoid errors\n",
        "            u = w / np.linalg.norm(w)\n",
        "            basis.append(u)\n",
        "    return np.array(basis)\n",
        "\n",
        "# Let's create an ONB from our 3D vectors x, y, z\n",
        "# These vectors span 3D space but are slanted/skewed relative to each other.\n",
        "raw_vectors = [vec_x, vec_y, vec_z]\n",
        "onb = gram_schmidt(raw_vectors)\n",
        "print(\"Original Skewed Vectors:\")\n",
        "print(np.array(raw_vectors))\n",
        "print(\"\\nNew Orthonormal Basis (Rows are basis vectors):\")\n",
        "print(onb)\n",
        "# Verification: Check dot product of first two basis vectors\n",
        "# If orthogonal, dot product should be close to 0\n",
        "dot_b1_b2 = np.dot(onb[0], onb[1])\n",
        "print(f\"\\nVerification: Dot product of basis 1 and 2: {dot_b1_b2:.10f} (Should be ~0)\")"
      ],
      "metadata": {
        "id": "z2U0dGU2AhG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd3ec983-cc71-4fa8-b5e2-8a16fcaacb8c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Skewed Vectors:\n",
            "[[ 1  0  0]\n",
            " [ 0  1  0]\n",
            " [ 1 -1  0]]\n",
            "\n",
            "New Orthonormal Basis (Rows are basis vectors):\n",
            "[[1. 0. 0.]\n",
            " [0. 1. 0.]]\n",
            "\n",
            "Verification: Dot product of basis 1 and 2: 0.0000000000 (Should be ~0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5.1: Verify the \"Normality\" (unit length) of the ONB generated above.\n",
        "\n",
        "**Activity:**\n",
        "\n",
        "Write a loop or use numpy commands to calculate the norm of each vector in the onb array.\n",
        "\n",
        "**Expected Result:**\n",
        "\n",
        "Each norm should be equal to 1.0 (within floating-point error).\n",
        "\n",
        "If they are not 1.0, the basis is orthogonal but not orthonormal."
      ],
      "metadata": {
        "id": "FlsKwzRIBC9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "EPwZ5vpYBFDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Orthogonal Complement**\n",
        "\n",
        "Imagine a flat sheet of paper floating in the middle of a room.\n",
        "\n",
        "- This paper is our subspace $U$.\n",
        "\n",
        "- Any arrow drawn directly on the paper is in $U$.\n",
        "\n",
        "- Any arrow sticking straight out of the paper (perpendicular to it) is in the Orthogonal Complement ($U^\\perp$).\n",
        "\n",
        "If you have a random arrow floating in the room (a data point), you can describe it as a combination of \"some movement along the paper\" plus \"some movement sticking straight out of the paper.\"\n",
        "\n",
        "The orthogonal complement represents everything the subspace misses.\n",
        "\n",
        "In data science, if your model explains the \"paper,\" the orthogonal complement is the \"error\" or \"residual\" that your model couldn't catch."
      ],
      "metadata": {
        "id": "c4O2OH94BV2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-TOPIC: ORTHOGONAL COMPLEMENT\n",
        "\n",
        "# We define a subspace U spanned by a single basis vector u1\n",
        "# For simplicity, U is just the X-axis line in 3D space\n",
        "u1 = np.array([1, 0, 0])\n",
        "# A vector v in the full 3D space\n",
        "v = np.array([3, 4, 5])\n",
        "\n",
        "\n",
        "# Decompose v into component in U (parallel) and component in U_perp (perpendicular)\n",
        "# Since u1 is the x-axis [1,0,0], the projection onto U is just the x-component of v.\n",
        "# Logic: We keep the part of v that matches u1's direction.\n",
        "v_parallel = np.array([3, 0, 0]) # Result: [3, 0, 0]\n",
        "# The orthogonal complement part is the remainder (v - v_parallel)\n",
        "# Logic: We take v and remove the part that was in U. What's left must be perpendicular.\n",
        "v_perp = v - v_parallel # Result: [0, 4, 5]\n",
        "\n",
        "print(f\"Original Vector v: {v}\")\n",
        "print(f\"Component in U (v_parallel): {v_parallel} (Signal/Model)\")\n",
        "print(f\"Component in U_perp (v_perp): {v_perp} (Noise/Residual)\")\n",
        "# Verification: Dot product should be 0\n",
        "check = np.dot(v_parallel, v_perp)\n",
        "print(f\"Dot Product Check: {check} (Proves orthogonality)\")"
      ],
      "metadata": {
        "id": "aX-YyaaEBlII",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef9856c5-d499-48e9-9c71-0aacee2aac50"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Vector v: [3 4 5]\n",
            "Component in U (v_parallel): [3 0 0] (Signal/Model)\n",
            "Component in U_perp (v_perp): [0 4 5] (Noise/Residual)\n",
            "Dot Product Check: 0 (Proves orthogonality)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Inner Product of Functions**\n",
        "\n",
        "We usually think of vectors as arrows. But functions (like curves on a graph) can act like vectors too.\n",
        "\n",
        "- Addition: We can add two curves to get a new curve.\n",
        "\n",
        "- Scaling: We can stretch a curve vertically.\n",
        "\n",
        "- Dot Product: We can take the \"dot product\" of two functions by multiplying them at every point and adding up all the results (which, in calculus, is finding the area under the curve via integration).\n",
        "\n",
        "If this \"area\" is zero, the functions are orthogonal. Even though they curve and wiggle, they cancel each other out perfectly.\n",
        "\n",
        "This is how noise-canceling headphones work: they create a sound wave that is the \"opposite\" of the noise, effectively summing to zero."
      ],
      "metadata": {
        "id": "r7tVQBaxCqa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-TOPIC: INNER PRODUCT OF FUNCTIONS\n",
        "def function_inner_product(func_f, func_g, interval):\n",
        "    \"\"\"\n",
        "    Computes inner product of two functions via numerical integration (Reimann Sum).\n",
        "    Formula: Integral(f(x) * g(x) dx) over [a, b]\n",
        "    \"\"\"\n",
        "    # Create 1000 points between start and end of interval\n",
        "    x_vals = np.linspace(interval[0], interval[1], 1000)\n",
        "    # Calculate the width of each tiny slice (dx)\n",
        "    dx = x_vals[1] - x_vals[0]\n",
        "    # Calculate f(x) * g(x) for all points\n",
        "    y_vals = func_f(x_vals) * func_g(x_vals)\n",
        "    # Sum up the area slices\n",
        "    integral = np.sum(y_vals) * dx\n",
        "    return integral\n",
        "\n",
        "# Define our functions: sin(x) and cos(x)\n",
        "f_sin = np.sin\n",
        "f_cos = np.cos\n",
        "# Interval [0, 2*pi] (One full wave cycle)\n",
        "interval_2pi = [0, 2*np.pi]\n",
        "val = function_inner_product(f_sin, f_cos, interval_2pi)\n",
        "print(f\"Inner Product of sin(x) and cos(x) over [0, 2pi]: {val:.5f}\")\n",
        "print(\"Result is approx 0. This proves sin and cos are orthogonal functions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofTjRnCqCyU4",
        "outputId": "7f21c7c1-86fa-4a46-eef0-39a192499caf"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inner Product of sin(x) and cos(x) over [0, 2pi]: 0.00000\n",
            "Result is approx 0. This proves sin and cos are orthogonal functions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 7.1:\n",
        "Calculate the inner product of $f(x) = 1$ (a constant horizontal line) and $g(x) = x$ (a diagonal line) over the interval $[-1, 1]$.\n",
        "\n",
        "Intuition:\n",
        "- $f(x)$ is symmetric (even).\n",
        "- $g(x)$ is antisymmetric (odd).\n",
        "- The area on the negative side (where $x$ is negative) should exactly cancel the area on the positive side.\n",
        "\n",
        "Prediction: *What should the result be?*\n",
        "\n",
        "Execution: Modify the code to test this hypothesis."
      ],
      "metadata": {
        "id": "Hhxw-RPqDEqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "zByhpj6uDLrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **8. Orthogonal Projections**\n",
        "\n",
        "- Imagine the sun is shining directly overhead.\n",
        "- You are standing on a flat field (the subspace).\n",
        "- Your Orthogonal Projection is your shadow on the ground.\n",
        "\n",
        "Closest Point:\n",
        "- Your shadow is the single point on the ground that is closest to your head. - Any other point on the ground is further away.\n",
        "\n",
        "Perpendicular:\n",
        "- The line connecting your head to your shadow’s head goes straight down, perpendicular to the ground.\n",
        "\n",
        "Compression:\n",
        "- In machine learning, we use projections to \"flatten\" complex, 3D data onto simpler 2D surfaces (like drawing a map of the Earth).\n",
        "- We want to do this flattening in a way that distorts the data as little as possible—keeping the \"shadow\" as accurate to the \"real object\" as we can."
      ],
      "metadata": {
        "id": "ecIVofvGIJz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-TOPIC: ORTHOGONAL PROJECTIONS\n",
        "\n",
        "def project_vector_onto_line(v, basis_vector):\n",
        "    \"\"\"\n",
        "    Projects vector v onto the line spanned by basis_vector.\n",
        "    Formula: proj_u(v) = ((v. u) / (u. u)) * u\n",
        "    This scales the basis vector 'u' by the amount of 'v' that points in its direction.\n",
        "    \"\"\"\n",
        "    # Numerator: Interaction between v and basis\n",
        "    numerator = np.dot(v, basis_vector)\n",
        "\n",
        "    # Denominator: Squared length of basis vector (normalization)\n",
        "    denominator = np.dot(basis_vector, basis_vector)\n",
        "\n",
        "    # The scalar projection (how \"long\" the shadow is)\n",
        "    scalar_proj = numerator / denominator\n",
        "\n",
        "    # The vector projection (the shadow itself)\n",
        "    vector_proj = scalar_proj * basis_vector\n",
        "\n",
        "    return vector_proj\n",
        "\n",
        "# Project vec_a (3,4) onto vec_b (1,2)\n",
        "# We want to find the shadow of 'a' falling on the line defined by 'b'\n",
        "proj_a_on_b = project_vector_onto_line(vec_a, vec_b)\n",
        "\n",
        "print(f\"Vector a: {vec_a}\")\n",
        "print(f\"Basis vector b: {vec_b}\")\n",
        "print(f\"Projection of a onto b: {proj_a_on_b}\")\n",
        "\n",
        "# Check error vector (original - projection)\n",
        "error_vec = vec_a - proj_a_on_b\n",
        "# The error should be orthogonal (90 degrees) to the basis b\n",
        "ortho_check = np.dot(error_vec, vec_b)\n",
        "print(f\"Orthogonality Check (Error. b): {ortho_check:.5f} (Should be 0)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3F6IaV-kJUlx",
        "outputId": "369c27db-5de8-4551-ca48-c77d139748d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector a: [3 4]\n",
            "Basis vector b: [1 2]\n",
            "Projection of a onto b: [2.2 4.4]\n",
            "Orthogonality Check (Error. b): -0.00000 (Should be 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 8.1 : Project vec_x onto the subspace spanned by the Z-axis.\n",
        "\n",
        "Intuition: Since you are projecting onto the Z-axis, the projection should effectively \"delete\" the X and Y components and keep only the Z component of the original vector.\n",
        "\n",
        "Execution: Use the function to compute the projection and verify the result."
      ],
      "metadata": {
        "id": "DcnD-HUqJmfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "ZeY-jv8YKBEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **9. Rotations**\n",
        "A rotation is simply spinning the entire data landscape (or the object itself) around a fixed point.\n",
        "\n",
        "- Rigid Motion:\n",
        "  - Imagine holding a picture frame.\n",
        "  - If you turn it 90 degrees, the picture doesn't get bigger or smaller (Length is preserved).\n",
        "  - The corners of the frame remain 90 degrees (Angles are preserved).\n",
        "  - The geometry is unchanged; only the orientation changes.\n",
        "\n",
        "- Invariance:\n",
        "  - In machine learning, we often want our AI to recognize a cat whether the cat is standing up or tilting its head.\n",
        "  - By applying rotations to our training data, we teach the AI that \"orientation doesn't change the identity of the object.\"\n",
        "\n",
        "- Perspective:\n",
        "  - Rotations allow us to change our perspective to find the \"best\" angle to look at the data, just like turning a diamond in the light to see its facets clearly."
      ],
      "metadata": {
        "id": "6Ke1pE1MKDeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUB-TOPIC: ROTATIONS\n",
        "\n",
        "def rotate_2d(vector, theta_degrees):\n",
        "    \"\"\"\n",
        "    Rotates a 2D vector by theta degrees counter-clockwise using a rotation matrix.\n",
        "    \"\"\"\n",
        "    # Convert degrees to radians (math functions expect radians)\n",
        "    theta_rad = np.radians(theta_degrees)\n",
        "\n",
        "    # Define the Rotation Matrix R\n",
        "    # Row 1: [cos, -sin]\n",
        "    # Row 2: [sin, cos]\n",
        "    c, s = np.cos(theta_rad), np.sin(theta_rad)\n",
        "    R = np.array(((c, -s), (s, c)))\n",
        "\n",
        "    # Apply transformation via matrix multiplication (Dot product)\n",
        "    v_rotated = np.dot(R, vector)\n",
        "    return v_rotated\n",
        "\n",
        "# Original Vector (1, 0) - points East\n",
        "v_east = np.array([1,0])\n",
        "\n",
        "# Rotate 90 degrees (should point North)\n",
        "v_north = rotate_2d(v_east, 90)\n",
        "\n",
        "# Rotate 45 degrees (North-East)\n",
        "v_ne = rotate_2d(v_east, 45)\n",
        "\n",
        "print(f\"Original: {v_east}\")\n",
        "print(f\"Rotated 90 deg: {v_north} (Expected: ~)\")\n",
        "print(f\"Rotated 45 deg: {v_ne}\")\n",
        "\n",
        "# Verify Length Conservation\n",
        "len_orig = np.linalg.norm(v_east)\n",
        "len_rot = np.linalg.norm(v_ne)\n",
        "print(f\"Length Original: {len_orig:.2f}, Length Rotated: {len_rot:.2f} (Should be equal)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLP4HAFqKGsK",
        "outputId": "6bd957fb-6d7e-49f1-cf7b-99a6657472fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: [1 0]\n",
            "Rotated 90 deg: [6.123234e-17 1.000000e+00] (Expected: ~)\n",
            "Rotated 45 deg: [0.70710678 0.70710678]\n",
            "Length Original: 1.00, Length Rotated: 1.00 (Should be equal)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Rotate the vector vec_a ($$) by 180 degrees.\n",
        "\n",
        "Expectation: Rotating by 180 degrees is equivalent to flipping the vector to the opposite side of the origin. It should point in the exact opposite direction.\n",
        "\n",
        "Prediction: *If the input is $$, what should the output be?*\n",
        "\n",
        "Execution: Run the code and confirm your prediction."
      ],
      "metadata": {
        "id": "8ACsLvftKje2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#WRITE CODE HERE"
      ],
      "metadata": {
        "id": "XWif93cQKowM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}